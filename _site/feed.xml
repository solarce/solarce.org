<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">solarce.org</title>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="/" />
<updated>2015-03-19T11:20:21-07:00</updated>
<id>/</id>
<author>
  <name>Brandon Burton</name>
  <uri>/</uri>
  <email>brandon@inatree.org</email>
</author>


<entry>
  <title type="html"><![CDATA[knife-block v0.2.0 released]]></title>
  <link rel="alternate" type="text/html" href="/writing/knife-block-v0.2.0/"/>
  <id>/writing/knife-block-v0.2.0</id>
  <published>2015-03-16T00:00:00-07:00</published>
  <updated>2015-03-16T00:00:00-07:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#chef" term="chef" /><category scheme="/tags/#knife" term="knife" /><category scheme="/tags/#knife-block" term="knife-block" />
  <content type="html">
    &lt;h1 id=&quot;knife-block-v021-released&quot;&gt;knife-block v0.2.1 released.&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;Important Update:&lt;/em&gt; v0.2.0 has been yanked because it wasn’t actually working on Chef12/ChefDK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/knife-block/knife-block/releases/tag/v0.2.1&quot;&gt;v0.2.1&lt;/a&gt; is out and on &lt;a href=&quot;https://rubygems.org/gems/knife-block/versions/0.2.1&quot;&gt;rubygems.org&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This release includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A fix so knife-block now works on Chef12&lt;/li&gt;
  &lt;li&gt;Some updates to the documentation&lt;/li&gt;
  &lt;li&gt;Some travis build improvements, including
    &lt;ul&gt;
      &lt;li&gt;Multiple 2.x Ruby builds&lt;/li&gt;
      &lt;li&gt;Building on 2.1 with chefdk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A shiny logo&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;the-future&quot;&gt;The Future&lt;/h1&gt;

&lt;p&gt;Some future improvements I’m hoping to make in the coming months are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Get the code base happy with Rubocop&lt;/li&gt;
  &lt;li&gt;Get CI builds going on OSX (w/ rvm and chefdk) and Windows builds&lt;/li&gt;
  &lt;li&gt;Start working on any Windows related bugs that crop up&lt;/li&gt;
  &lt;li&gt;Better integration with Berkshelf config stuff&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/knife-block-v0.2.0/&quot;&gt;knife-block v0.2.0 released&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on March 16, 2015.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[tmux.conf]]></title>
  <link rel="alternate" type="text/html" href="/writing/tmux-conf/"/>
  <id>/writing/tmux-conf</id>
  <published>2014-12-20T00:00:00-08:00</published>
  <updated>2014-12-20T00:00:00-08:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#chef" term="chef" /><category scheme="/tags/#knife" term="knife" /><category scheme="/tags/#knife-block" term="knife-block" />
  <content type="html">
    &lt;h1 id=&quot;tmuxconf&quot;&gt;tmux.conf&lt;/h1&gt;

&lt;p&gt;I spent a while this morning putting together a new &lt;a href=&quot;https://github.com/solarce/dotfiles/blob/master/tmux.conf&quot;&gt;tmux.conf&lt;/a&gt; and I’ve added it to my &lt;a href=&quot;https://github.com/solarce/dotfiles&quot;&gt;dotfiles repository&lt;/a&gt; on Github.&lt;/p&gt;

&lt;p&gt;I’m gonna try to re-organize my &lt;a href=&quot;http://www.solarce.org/tips/tmux.html&quot;&gt;tmux tips&lt;/a&gt; page around it here soon.&lt;/p&gt;

&lt;p&gt;Let me know what you think on &lt;a href=&quot;https://twitter.com/solarce&quot;&gt;Twitter&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/tmux-conf/&quot;&gt;tmux.conf&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on December 20, 2014.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Aws Elb Backend Auth With The Cli]]></title>
  <link rel="alternate" type="text/html" href="/aws-elb-backend-auth-with-the-cli/"/>
  <id>/aws-elb-backend-auth-with-the-cli</id>
  <published>2014-12-11T00:00:00-08:00</published>
  <updated>2014-12-11T00:00:00-08:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  
  <content type="html">
    

    &lt;p&gt;&lt;a href=&quot;/aws-elb-backend-auth-with-the-cli/&quot;&gt;Aws Elb Backend Auth With The Cli&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on December 11, 2014.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Helping with knife-block]]></title>
  <link rel="alternate" type="text/html" href="/writing/helping-with-knife-block/"/>
  <id>/writing/helping-with-knife-block</id>
  <published>2014-12-10T00:00:00-08:00</published>
  <updated>2014-12-10T00:00:00-08:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#chef" term="chef" /><category scheme="/tags/#knife" term="knife" /><category scheme="/tags/#knife-block" term="knife-block" />
  <content type="html">
    &lt;h1 id=&quot;helping-with-knife-block&quot;&gt;Helping with knife-block&lt;/h1&gt;

&lt;p&gt;We make heavy use of &lt;a href=&quot;https://github.com/knife-block/knife-block&quot;&gt;knife-block&lt;/a&gt; in &lt;a href=&quot;https://twitter.com/lookout&quot;&gt;Lookout Operations&lt;/a&gt; and yesterday we encountered &lt;a href=&quot;https://github.com/knife-block/knife-block/issues/25&quot;&gt;an issue with it and Chef 12’s knife&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I really appreciate all the work &lt;a href=&quot;https://twitter.com/ProfFalken&quot;&gt;Matthew Macdonald-Wallace&lt;/a&gt; put into making knife-block and also his transparency in not having time/need to work on it anymore.&lt;/p&gt;

&lt;p&gt;Last night I &lt;a href=&quot;https://twitter.com/ProfFalken/status/542607454178406400&quot;&gt;reached out to him&lt;/a&gt; about moving knife-block to its own &lt;a href=&quot;https://github.com/knife-block/&quot;&gt;Github org&lt;/a&gt; and taking over maintainence of the project. He was &lt;a href=&quot;https://twitter.com/ProfFalken/status/542608021097308160&quot;&gt;awesome enough to say yes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re an active user of knife-block, and especially if you’d like to help maintain it, &lt;a href=&quot;https://twitter.com/&quot;&gt;let me know on Twitter!&lt;/a&gt; or file &lt;a href=&quot;https://github.com/knife-block/knife-block/issues&quot;&gt;Github issues&lt;/a&gt;.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/helping-with-knife-block/&quot;&gt;Helping with knife-block&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on December 10, 2014.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[Experimenting with Exhibitor/Zookeeper and Kafka on CoreOS]]></title>
  <link rel="alternate" type="text/html" href="/writing/experiments-with-zookeeper-kafka-coreos/"/>
  <id>/writing/experiments-with-zookeeper-kafka-coreos</id>
  <published>2014-11-25T00:00:00-08:00</published>
  <updated>2014-11-25T00:00:00-08:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#zookeeper" term="zookeeper" /><category scheme="/tags/#exhibitor" term="exhibitor" /><category scheme="/tags/#kafka" term="kafka" /><category scheme="/tags/#coreos" term="coreos" /><category scheme="/tags/#docker" term="docker" />
  <content type="html">
    &lt;h1 id=&quot;experimenting-with-coreos-zookeeper-exhibitor-and-kafka-for-hacksgiving-2014&quot;&gt;Experimenting with CoreOS, Zookeeper, Exhibitor, and Kafka for Hacksgiving 2014&lt;/h1&gt;

&lt;p&gt;As part of &lt;a href=&quot;http://hacksgiving.com/2012/11/what-is-hacksgiving/&quot;&gt;Hacksgiving 2014&lt;/a&gt; at &lt;a href=&quot;https://www.lookout.com&quot;&gt;Lookout&lt;/a&gt; I experimented with seeing how hard it would be to get &lt;strong&gt;Zookeeper&lt;/strong&gt; and &lt;strong&gt;Kafka&lt;/strong&gt; working directly on &lt;a href=&quot;https://coreos.com&quot;&gt;CoreOS&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deploying-coreos&quot;&gt;Deploying CoreOS&lt;/h2&gt;

&lt;p&gt;Deploying CoreOS was pretty easy. I opted to deploy the latest stable release and use their &lt;a href=&quot;https://s3.amazonaws.com/coreos.com/dist/aws/coreos-stable-hvm.template&quot;&gt;CloudFormation&lt;/a&gt; template as a starting place.&lt;/p&gt;

&lt;p&gt;I customized it a bit to support being inside our VPC and just use our default security groups.&lt;/p&gt;

&lt;p&gt;First I got a new &lt;a href=&quot;https://coreos.com/docs/cluster-management/setup/cluster-discovery/&quot;&gt;discovery token for etcd&lt;/a&gt;, with &lt;code&gt;curl https://discovery.etcd.io/new&lt;/code&gt; and then I filled in the other parameters in the CloudFormation template I made. I’ve put &lt;a href=&quot;https://github.com/solarce/coreos_zk_kafka_notes/blob/master/coreos_cfn_template.json&quot;&gt;an example template on Github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then I just used the &lt;a href=&quot;https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new&quot;&gt;CloudFormation GUI&lt;/a&gt; in the AWS Console to launch a stack with my template. This built an &lt;a href=&quot;http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WhatIsAutoScaling.html&quot;&gt;Auto-Scaling Group&lt;/a&gt; and then launched three EC2 instances for me.&lt;/p&gt;

&lt;p&gt;Once all the instances were launched and I confirmed I could SSH into them, I added DNS records for them.&lt;/p&gt;

&lt;h2 id=&quot;getting-zookeeper-working&quot;&gt;Getting Zookeeper working&lt;/h2&gt;

&lt;p&gt;Launching services on CoreOS is pretty easy to get going, as it uses &lt;a href=&quot;https://docker.io/&quot;&gt;Docker&lt;/a&gt; containers to &lt;em&gt;package&lt;/em&gt; the services and &lt;a href=&quot;https://coreos.com/docs/launching-containers/launching/fleet-unit-files/&quot;&gt;unit files&lt;/a&gt; to define how a service should be run. But because CoreOS is designed to run more ephemeral workloads, it’s kind of a challenge to run something like &lt;a href=&quot;https://zookeeper.apache.org&quot;&gt;Zookeeper&lt;/a&gt; by itself. Luckily the folks at Netflix released &lt;a href=&quot;https://github.com/Netflix/exhibitor/wiki/Running-Exhibitor&quot;&gt;Exhibitor&lt;/a&gt; to tackle this kind of challenge and uses S3 to share a config file for the Zookeeper cluster. So I made an S3 bucket that the IAM credentials I was going to use could write to.&lt;/p&gt;

&lt;p&gt;Then I found a docker image that &lt;a href=&quot;https://github.com/thefactory/docker-zk-exhibitor&quot;&gt;packaged exhibitor and zookeeper together&lt;/a&gt; and I wrote a unit file to launch it. I then launched this with &lt;a href=&quot;https://coreos.com/docs/launching-containers/launching/launching-containers-fleet/&quot;&gt;fleetctl&lt;/a&gt;, which is the tool CoreOS built on top of &lt;a href=&quot;http://www.freedesktop.org/software/systemd/&quot;&gt;systemd&lt;/a&gt; to manage launching services across a CoreOS cluster.&lt;/p&gt;

&lt;p&gt;The unit file looked like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=ex-zk1
After=docker.service
Requires=docker.service

[Service]
Type=Simple
TimeoutStartSec=0
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill ex-zk1
ExecStartPre=-/usr/bin/docker rm ex-zk1
ExecStartPre=/usr/bin/docker pull thefactory/zookeeper-exhibitor:latest
ExecStart=/usr/bin/docker run --name ez-zk1 \
                          -p 8181:8181 \
                          -p 2181:2181 \
                          -p 2888:2888 \
                          -p 3888:3888 \
                          -e S3_BUCKET=&amp;lt;BUCKET_NAME&amp;gt; \
                          -e S3_PREFIX=&amp;lt;KEY_NAME&amp;gt; \
                          -e AWS_ACCESS_KEY_ID=&amp;lt;ACCESS_KEY&amp;gt; \
                          -e AWS_SECRET_ACCESS_KEY=&amp;lt;SECRET_KEY&amp;gt; \
                          -e HOSTNAME=ez-zk1 \
                          thefactory/zookeeper-exhibitor:latest
ExecStop=/usr/bin/docker stop ex-zk1
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;a-single-zookeeper&quot;&gt;A single zookeeper!&lt;/h3&gt;

&lt;p&gt;I got a single node up and running pretty easily with the following commands:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;fleetctl submit ex-zk1.service&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;fleetctl start ex-zk1&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;fleetctl status ex-zk1&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I was then able to view the Exhibitor UI.&lt;/p&gt;

&lt;h3 id=&quot;a-cluster-of-zookeepers&quot;&gt;A cluster of zookeepers!&lt;/h3&gt;

&lt;p&gt;A single zookeeper was easy, but when I launched three of them with the first iteration of my unit file, I found that each zookeeper instance would stick in standalone mode, despite them all being able to write to my S3 bucket.&lt;/p&gt;

&lt;p&gt;After some troubleshooting I realized this was because the hostname I was giving to each container was not resolvable by the other containers, so the zookeeper services couldn’t actually communicate and perform leader/follower elections. Since docker is binding the ports of each container to the IP of the CoreOS hosts, I found that if I told each container it had the hostname of it’s CoreOS host, then election occured and Exhibitor was able to get all three members in sync and happy.&lt;/p&gt;

&lt;p&gt;So the unit files then looked like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=ex-zk1
After=docker.service
Requires=docker.service

[Service]
Type=Simple
TimeoutStartSec=0
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill ex-zk1
ExecStartPre=-/usr/bin/docker rm ex-zk1
ExecStartPre=/usr/bin/docker pull thefactory/zookeeper-exhibitor:latest
ExecStart=/usr/bin/docker run --name ez-zk1 \
                          -p 8181:8181 \
                          -p 2181:2181 \
                          -p 2888:2888 \
                          -p 3888:3888 \
                          -e S3_BUCKET=&amp;lt;BUCKET_NAME&amp;gt; \
                          -e S3_PREFIX=&amp;lt;KEY_NAME&amp;gt; \
                          -e AWS_ACCESS_KEY_ID=&amp;lt;ACCESS_KEY&amp;gt; \
                          -e AWS_SECRET_ACCESS_KEY=&amp;lt;SECRET_KEY&amp;gt; \
                          -e HOSTNAME=&amp;lt;DNS_NAME_OF_COREOS_HOST&amp;gt; \
                          thefactory/zookeeper-exhibitor:latest
ExecStop=/usr/bin/docker stop ex-zk1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To deploy two more nodes I just copied the &lt;code&gt;ex-zk1.service&lt;/code&gt; file and tweaked the names and &lt;code&gt;HOSTNAME&lt;/code&gt; to match where fleetctl would distribute the containers. I then launched &lt;code&gt;ex-zk2&lt;/code&gt; and &lt;code&gt;ex-zk3&lt;/code&gt; and had a three node cluster running.&lt;/p&gt;

&lt;p&gt;Obviously this isn’t ideal and support for more complex networking on CoreOS would be needed for a more production-like deployment, but for the purposes of this Hacksgiving it got what I wanted.&lt;/p&gt;

&lt;h2 id=&quot;getting-kafka-working&quot;&gt;Getting Kafka working&lt;/h2&gt;

&lt;p&gt;The end goal of my experimenting was a working three node Kafka cluster, but most of the work was getting Zookeeper working, because Kafka uses Zookeeper for its cluster discovery and leader election. So like with zookeeper, I found a docker image that looked good and made a unit file for it, like the one shown below:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=kafka1
After=docker.service
Requires=docker.service

[Service]
Type=Simple
TimeoutStartSec=0
EnvironmentFile=/etc/environment
ExecStartPre=-/usr/bin/docker kill kafka1
ExecStartPre=-/usr/bin/docker rm kafka1
ExecStartPre=/usr/bin/docker pull wurstmeister/kafka:0.8.1.1-1
ExecStart=/usr/bin/docker run --name kafka1 \
                          -p 6667:6667 \
                          -e KAFKA_ADVERTISED_PORT=6667 \
                          -e KAFKA_ZOOKEEPER_CONNECT=10.1.1.10:2181,10.1.1.12:2181,10.2.1.15:2181 \
                          -e KAFKA_BROKER_ID=100 \
                          wurstmeister/kafka:0.8.1.1-1
ExecStop=/usr/bin/docker stop kafka1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The main downside to the above unit file is that I’m hardcoding my zookeeper configs based on the IPs of the CoreOS hosts, ideally we’d tap into &lt;a href=&quot;https://coreos.com/docs/distributed-configuration/getting-started-with-etcd/&quot;&gt;etcd&lt;/a&gt; or something to manage this more dynamically.&lt;/p&gt;

&lt;p&gt;Once I started one instance and confirmed it was up with &lt;code&gt;fleetctl&lt;/code&gt; I launched two more and confirmed they all joined the cluster by checking the contents of &lt;code&gt;/brokers/ids&lt;/code&gt; in zookeeper. Then I tested making a topic with three &lt;a href=&quot;https://confluence.corp.lookout.com/display/eng/Kafka#Kafka-Partitions&quot;&gt;topic partitions&lt;/a&gt; and a &lt;a href=&quot;https://confluence.corp.lookout.com/display/eng/Kafka#Kafka-ReplicationFactor&quot;&gt;replication factor&lt;/a&gt; of three. This would confirm that the cluster was truly working as expected.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;bburton@althalus ~ # kafka-topics.sh --create --zookeeper coreos-1 --replication-factor 3 --partitions 3 --topic bburton.test3
Created topic &quot;bburton.test3&quot;.
bburton@althalus ~ # kafka-topics.sh --describe --zookeeper lookout-coreos-hacksgiving1.flexilis.org
Topic:bburton.test3     PartitionCount:3        ReplicationFactor:3     Configs:
        Topic: bburton.test3    Partition: 0    Leader: 300     Replicas: 300,200,100   Isr: 300,200,100
        Topic: bburton.test3    Partition: 1    Leader: 100     Replicas: 100,300,200   Isr: 100,300,200
        Topic: bburton.test3    Partition: 2    Leader: 200     Replicas: 200,100,300   Isr: 200,100,300
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;That’s pretty much it for what I did during the hackathon!&lt;/p&gt;

&lt;p&gt;I put up &lt;a href=&quot;https://github.com/solarce/coreos_zk_kafka_notes&quot;&gt;samples of my unit files and the cloudformation template on Github as&lt;/a&gt; well.&lt;/p&gt;

&lt;h2 id=&quot;ps-taking-it-further&quot;&gt;PS. Taking it further&lt;/h2&gt;

&lt;p&gt;Some thoughts on how we could take it further. We’d need to figure out things like:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Durable Storage&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;http://docs.aws.amazon.com/AutoScaling/latest/DeveloperGuide/WorkingWithLaunchConfig.html&quot;&gt;Launch Configuration&lt;/a&gt; I used didn’t mount the ephemeral drives of each instance, nor did it add any EBS volumes. Additionally, the unit files I made don’t use any &lt;a href=&quot;https://docs.docker.com/userguide/dockervolumes/&quot;&gt;docker volume mounts&lt;/a&gt; to let container data persist when instances are recreated.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Complex Networking&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Because of how the unit files are wrote are doing &lt;a href=&quot;https://docs.docker.com/userguide/dockerlinks/&quot;&gt;docker port binding&lt;/a&gt; we can only support one kind of service port CoreOS host. Figuring out more complex networking would allow us to support multiple services in different in clusters on the same CoreOS hosts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Unit File Management&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More research into best practices for versioning and deploying unit files is needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Thinking in a docker world&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using CoreOS brings docker and many things have to be rethought when in a docker world, such as
    &lt;ul&gt;
      &lt;li&gt;SSH&lt;/li&gt;
      &lt;li&gt;Logging&lt;/li&gt;
      &lt;li&gt;Storage Persistance&lt;/li&gt;
      &lt;li&gt;Etc&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Docker Image Hosting&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Would we want to run a private registry to build and host our own docker containers?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Monitoring of CoreOS and Containers&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;How do we best monitor CoreOS and the containers we run on it?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Where does Chef fit in?&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Building containers from Chef (and something like &lt;a href=&quot;https://packer.io&quot;&gt;Packer&lt;/a&gt;?)&lt;/li&gt;
  &lt;li&gt;Deploying CoreOS with something like &lt;a href=&quot;https://github.com/opscode/chef-provisioning&quot;&gt;chef-provisioning&lt;/a&gt;?&lt;/li&gt;
&lt;/ul&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/experiments-with-zookeeper-kafka-coreos/&quot;&gt;Experimenting with Exhibitor/Zookeeper and Kafka on CoreOS&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on November 25, 2014.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[The technologist's prayer]]></title>
  <link rel="alternate" type="text/html" href="/writing/technologists-prayer/"/>
  <id>/writing/technologists-prayer</id>
  <published>2013-05-22T00:00:00-07:00</published>
  <updated>2013-05-22T00:00:00-07:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#technologist" term="technologist" /><category scheme="/tags/#prayer" term="prayer" /><category scheme="/tags/#yak-shaving" term="yak-shaving" />
  <content type="html">
    &lt;blockquote&gt;
  &lt;p&gt;God, grant me the serenity to accept the &lt;a href=&quot;http://projects.csail.mit.edu/gsb/old-archive/gsb-archive/gsb2000-02-11.html&quot;&gt;yak shaves&lt;/a&gt; I cannot change, the courage to avoid the yaks I can, and wisdom to know the difference.&lt;/p&gt;
&lt;/blockquote&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/technologists-prayer/&quot;&gt;The technologist's prayer&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on May 22, 2013.&lt;/p&gt;
  </content>
</entry>


<entry>
  <title type="html"><![CDATA[The future of monitoring is a toolbox]]></title>
  <link rel="alternate" type="text/html" href="/writing/future-of-monitoring-toolbox/"/>
  <id>/writing/future-of-monitoring-toolbox</id>
  <published>2012-02-12T00:00:00-08:00</published>
  <updated>2012-02-12T00:00:00-08:00</updated>
  <author>
    <name>Brandon Burton</name>
    <uri></uri>
    <email>brandon@inatree.org</email>
  </author>
  <category scheme="/tags/#monitoring" term="monitoring" />
  <content type="html">
    &lt;p&gt;&lt;em&gt;Originally written on February 12th 2012&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There’s been a small flurry of discussion today on Twitter and IRC, due to comments from folks who, in response to an &lt;a href=&quot;http://gigaom.com/2012/02/12/why-monitoring-sucks-for-now&quot;&gt;article&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/#!/moonpolysoft&quot;&gt;Cliff Moon&lt;/a&gt;, seem to think that &lt;em&gt;#monitoringsucks&lt;/em&gt; is some kind of &lt;a href=&quot;http://gigaom.com/2012/02/12/why-monitoring-sucks-for-now/#comment-808804&quot;&gt;marketing&lt;/a&gt; &lt;a href=&quot;https://twitter.com/#!/parkercloud/status/168744558589521920&quot;&gt;bullshit&lt;/a&gt;. And that just doesn’t sit right with me, so I hope to set the record straight.Now, I don’t claim to speak for &lt;em&gt;#monitoringsucks&lt;/em&gt; as a whole or represent all the views. However, as an active member of the devops/webops/IT community through Twitter, IRC, blogging, conferences, etc, I think I can speak with some confidence on the topic. Of course, I invite people to respond to this post, via email, Twitter, or on your own blog and correct me where I am wrong or let me know where you disagree. Let’s start at what should be a foregone conclusion. &lt;strong&gt;Monitoring sucks&lt;/strong&gt;. It does. The majority of monitoring systems out there are built around workloads and change processes that are no longer valid for those at the leading edge of things and will continue to become so for more and more of the industry. The future of application environments is dynamic, scalable, and ever changing. It’s a polyglot of language runtimes and operating systems. And in many cases it’s a mixture of IaaS, PaaS, and in-house computing resources. Given that, the idea that monitoring can be a swiss army knife, and a swiss army knife that’s mostly SNMP, curl, and ping, is dead. The future of monitoring is a tool box, and right now, most of the monitoring tools available to us suck, in light of the future I just outlined. So that’s why &lt;em&gt;#monitoringsucks&lt;/em&gt;So where does &lt;em&gt;#monitoringsucks&lt;/em&gt; come from? It comes from the collective realization of the operations community that the future is now and that we need to be outspoken about the future and help our community and industry adapt. You don’t have to look far to see this realization over and over, as evidenced by numerous blog posts, such as&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://lusislog.blogspot.com/2011/06/why-monitoring-sucks.html&quot;&gt;Why monitoring sucks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lusislog.blogspot.com/2011/07/monitoring-sucks-watch-your-language.html&quot;&gt;Monitoring Sucks – Watch your language&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://obfuscurity.com/2011/07/Monitoring-Sucks-Do-Something-About-It&quot;&gt;Monitoring Sucks. Do Something About It.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Monitoring Sucks. Latency Sucks More. &lt;a href=&quot;http://holmwood.id.au/~lindsay/2012/01/09/monitoring-sucks-latency-sucks-more/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;http://holmwood.id.au/~lindsay/2012/01/11/monitoring-system-equal-web-app-when-diagnosing-performance-bottlenecks/&quot;&gt;part 2&lt;/a&gt;, and &lt;a href=&quot;http://holmwood.id.au/~lindsay/2012/01/11/monitoring-system-equal-web-app-when-diagnosing-performance-bottlenecks/&quot;&gt;part 3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.socallinuxexpo.org/scale10x/presentations/panel-monitoring-sucks&quot;&gt;Monitoring Sucks Panel at SCaLe 10&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.krisbuytaert.be/blog/we-didnt-fix-it&quot;&gt;The #monitoringsucks HackFest after FOSDEM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;And of course, &lt;a href=&quot;https://twitter.com/#!/patrickdebois&quot;&gt;Patrick Debois’s&lt;/a&gt; epic series – Monitoring Wonderland Survey&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jedi.be/blog/2012/01/03/monitoring-wonderland-survey-introduction/&quot;&gt;Monitoring Wonderland Survey – Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jedi.be/blog/2012/01/03/monitoring-wonderland-metrics-api-gateways/&quot;&gt;Monitoring Wonderland Survey – Metrics – API – Gateways&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jedi.be/blog/2012/01/03/monitoring-wonderland-nagios-the-mighty-beast/&quot;&gt;Monitoring Wonderland Survey – Nagios the Mighty Beast&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jedi.be/blog/2012/01/04/monitoring-wonderland-moving-up-the-stack-application-user-metrics/&quot;&gt;Monitoring Wonderland Survey – Moving up the stack Application and User metric&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jedi.be/blog/2012/01/04/monitoring-wonderland-visualization/&quot;&gt;Monitoring Wonderland Survey – Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I could go on and on with links to posts that are either addressing the root problems of why &lt;em&gt;#monitoringsucks&lt;/em&gt;, helping survey the current tools, or describing new systems being built to help address the future and make monitoring suck a little bit less, but the point of this article isn’t to be a link dump, it’s to help educate on what &lt;em&gt;#monitoringsucks&lt;/em&gt; really is and really isn’t. Of course, you may ask “now that’ve you’ve bitched, what are you doing about it?”, which is an excellent and reasonable question.&lt;/p&gt;

&lt;p&gt;So what is the community doing about, besides saying &lt;em&gt;monitoring sucks!&lt;/em&gt;? We are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Discussing is on twitter with the &lt;a href=&quot;https://twitter.com/#!/search/%23monitoringsucks&quot;&gt;#monitoringsucks&lt;/a&gt; hashtag&lt;/li&gt;
  &lt;li&gt;Discussing it at conferences, like &lt;a href=&quot;ttp://www.socallinuxexpo.org/scale10x/presentations/panel-monitoring-sucks&quot;&gt;SCaLE&lt;/a&gt; or &lt;a href=&quot;http://www.krisbuytaert.be/blog/we-didnt-fix-it&quot;&gt;FOSDEM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Discussing it on &lt;a href=&quot;irc://irc.freenode.net/#monitoringsucks&quot;&gt;IRC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Providing centralized places to help people find tools for their toolboxes, like the &lt;a href=&quot;https://github.com/monitoringsucks/tool-repos&quot;&gt;Github Tools Repo&lt;/a&gt; and a handy &lt;a href=&quot;http://monitoring.no.de/&quot;&gt;web interface to the repo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Building new tools, some commercial, some open source, like &lt;a href=&quot;http://circonus.com/&quot;&gt;Circonus&lt;/a&gt;, &lt;a href=&quot;https://boundary.com/&quot;&gt;Boundary&lt;/a&gt;, &lt;a href=&quot;https://metrics.librato.com/&quot;&gt;Librato Metrics&lt;/a&gt;, &lt;a href=&quot;http://graphite.wikidot.com/&quot;&gt;Graphite&lt;/a&gt;, or &lt;a href=&quot;http://www.sonian.com/cloud-monitoring-sensu/&quot;&gt;Sensu&lt;/a&gt; that are built from the ground up to be ready for future that is fast becoming reality for operations folks&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So &lt;em&gt;#monitoringsucks&lt;/em&gt; isn’t some marketing bullshit meant to help shine a light on Boundary, it’s a real and correct topic that is being driven by the community to help better the state of monitoring. And if you find fault in Cliff for writing an article on it, because the company he is a cofounder of is trying to help address things, while also trying to make money, I think that’s a load of horseshit. I applaud the folks at &lt;a href=&quot;http://circonus.com/&quot;&gt;Circonus&lt;/a&gt;, &lt;a href=&quot;https://boundary.com/&quot;&gt;Boundary&lt;/a&gt;, &lt;a href=&quot;https://metrics.librato.com/&quot;&gt;Librato Metrics&lt;/a&gt;, and other companies that are helping to improve the state of things, but also making money at it, because the idea that just because someone is trying to make money in something, automatically discredits them due to &lt;em&gt;bias&lt;/em&gt; or whatever, is crap, especially in this case, where if you spend even 15 minutes to see what &lt;em&gt;#monitoringsucks&lt;/em&gt; is about, you’ll see it’s from the community. Quite frankly, I think a lot of the resistance things like &lt;em&gt;#monitoringsucks&lt;/em&gt; or &lt;em&gt;devops&lt;/em&gt; get from certain people is just those people trying to push their own &lt;em&gt;brand&lt;/em&gt; by being a &lt;em&gt;thought leader&lt;/em&gt;, but that’s a topic for another post.&lt;/p&gt;

    &lt;p&gt;&lt;a href=&quot;/writing/future-of-monitoring-toolbox/&quot;&gt;The future of monitoring is a toolbox&lt;/a&gt; was originally published by Brandon Burton at &lt;a href=&quot;&quot;&gt;solarce.org&lt;/a&gt; on February 12, 2012.&lt;/p&gt;
  </content>
</entry>

</feed>
